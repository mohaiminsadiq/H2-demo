{% extends 'base.html' %}

{% block head %}
    <link rel="stylesheet" href="{{ url_for('static', filename='css/results_burden.css') }}">
{% endblock %}


{% block content %}
<style>
h1 {text-align: left;}
h2 {text-align: left;}
p {text-align: left;}

.centered{
      display : block;
      margin : 0 auto;
      text-align: center;
      width: 800px;
      }

</style>

<div class="container"> 

<h1> Burden </h1>
<h2> Detecting Bias </h2>

<p> 
    Burden is a measure of how difficult it is for individuals in a certain group to get recourse and change from one prediction to another. It is an average measure of distance between an individual and their counter factual <maybe link to the info page on cfacts>. An important thing to note is that burden is a method of aggregating measures of individual fairness into a measure of group fairness Burden relies on a genetic algorithm<maybe link to genetic algo page> to generate counter factuals and then takes the average across groups. To detect bias using burden we look at how different the averages are between the groups. If the burden is higher for one group than the other then that is a clear indication that the model is biased against that group. 
    
    Looking at the plots below we first see a plot of the difference between burdens (average distance). This is followed by a PMF of the distances where lower is better. Finally, we have a scatter plot of all the individual distances where higher is worse. 
    
    the plots are interactive so take some time to move around and decide for yourself if there is any bias between the two groups.
</p>

<div class="centered">
    {{ burden_plt | safe }}
    {{ burden_src | safe }}
</div>

<div class="centered">
    {{ burden_pmf_plt | safe }}
    {{ burden_pmf_src | safe }}
</div>

<div class="centered">
    {{ scat_plt | safe }}
    {{ scat_src | safe }}
</div>

<h2>Discrimination Criteria</h2>
<p>
    Now that we have some idea about whether there is or isn't bias in the dataset with respect to the desired protected attribute we can move onto using burden to show the presence or absence of bias using two well-defined discrimination criteria: demographic parity and equalized odds.
</p>

<h1> Demographic Parity </h1>
<p>
    Demographic parity requires the probability of the model assigning a positive outcome (Ŷ=1) for each instance of each protected attribute (A) to be the same. In mathematical terms this is: 
</p>
    
<p style="text-align: center;"> <b>(Ŷ=1|A=1) = P(Ŷ=1|A=0)</b> </p>
<p>
    Intuitively this simply means that the probability of an instance from class A=1 being assigned a positive outcome by the model is equal to the probability of an instance from class A=0 being assigned a positive outcome by the model. 
</p>

<p>
    So one way to measure bias is the difference between these two probabilities. Looking below we can see how these probabilities compare after different bias mitigation techniques have been applied to derived predictors.
</p>

<div class="centered">
    {{ demo_plt | safe }}
    {{ demo_src | safe }}
</div>

<h1> Raw Results </h1>
<h2> Derived Predictors </h2>
<p>
    Now that we have detected and shown bias using the discrimination criterion with the help of an XAI technique we can move on to leveraging the explanations to mitigate the bias. There is a post-processing technique involving what are called “derived predictors” to achieve equalized odds. Essentially you take some of the predictions of the original model and flip them. How many predictions you flip (what is called the "mixing rate") are determined using convex optimization. In the traditional algorithm, which predictions you flip are randomly determined. This does achieve equalized odds on the races as a whole but it may be unfair to individual people. In the burden-based method, the counterfactual distance are used to select the correct individuals to flip, resulting in a higher degree of individual fairness. The table shows how the Burden-based method and Partial-Burden-based method performed versus the randomized algorithm.  
</p>
<div class="centered">
    {{ burden_table | safe }}
</div>
</div>
{% endblock %}
